{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Lens in RL Framework\n",
    "\n",
    "*State*: as the browsing history of a user, i.e., previous $N$ items\n",
    "that a user liked before time $t$\n",
    "\n",
    "\n",
    "DCF:\n",
    "https://towardsdatascience.com/deep-learning-based-recommender-systems-3d120201db7e\n",
    "\n",
    "DCF will output if recommend or not.\n",
    "\n",
    "Co-Trained-RL will tell which subset to sample from to maximize probability of DCF\n",
    "\n",
    "Shortcomings: Can only evaluate in Offline fashion...\n",
    "\n",
    "User eval: online simulator... would need to simulate user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_ml100k():\n",
    "    data_dir = 'ml-100k/'\n",
    "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    data = pd.read_csv(os.path.join(data_dir, 'u.data'), '\\t', names=names,\n",
    "                       engine='python')\n",
    "    num_users = data.user_id.unique().shape[0]\n",
    "    num_items = data.item_id.unique().shape[0]\n",
    "    return data, num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_item_data_ml100k():\n",
    "    data_dir = 'ml-100k/'\n",
    "    names = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "              'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "              \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "              'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "              'Thriller', 'War', 'Western']\n",
    "    data = pd.read_csv(os.path.join(data_dir, 'u.item'), '|', names=names,\n",
    "                       engine='python')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, num_users, num_items = read_data_ml100k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = read_item_data_ml100k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create item_data dictionary to get movie_id feature vectors\n",
    "movie_to_onehot = {}\n",
    "for i,row in item_data.iterrows():\n",
    "    movie_to_onehot[row['movie id']] = row[5:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(272)\n",
    "\n",
    "\n",
    "rand_userIds = np.random.choice(data['user_id'].unique(), \n",
    "                                size=int(len(data['user_id'].unique())*0.8), \n",
    "                                replace=False)\n",
    "\n",
    "data_30 = data#.loc[data['user_id'].isin(rand_userIds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave one out train-test split\n",
    "\n",
    "To avoid look-ahead bias/data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_30['rank_latest'] = data_30.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "\n",
    "train_ratings = data_30[data_30['rank_latest'] != 1]\n",
    "test_ratings = data_30[data_30['rank_latest'] == 1]\n",
    "\n",
    "# drop columns that we no longer need\n",
    "train_ratings = train_ratings[['user_id', 'item_id', 'rating']]\n",
    "test_ratings = test_ratings[['user_id', 'item_id', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Dataset to Implicit Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ratings.loc[:, 'rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieLensTrainDataset(Dataset):\n",
    "    \"\"\"MovieLens PyTorch Dataset for Training\n",
    "    \n",
    "    Args:\n",
    "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
    "        all_movieIds (list): List containing all movieIds\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratings, all_movieIds):\n",
    "        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "    def get_dataset(self, ratings, all_movieIds):\n",
    "        users, items, labels = [], [], []\n",
    "        user_item_set = set(zip(ratings['user_id'], ratings['item_id']))\n",
    "\n",
    "        num_negatives = 4\n",
    "        for u, i in user_item_set:\n",
    "            users.append(u)\n",
    "            items.append(i)\n",
    "            labels.append(1)\n",
    "            for _ in range(num_negatives):\n",
    "                negative_item = np.random.choice(all_movieIds)\n",
    "                while (u, negative_item) in user_item_set:\n",
    "                    negative_item = np.random.choice(all_movieIds)\n",
    "                users.append(u)\n",
    "                items.append(negative_item)\n",
    "                labels.append(0)\n",
    "\n",
    "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = data_30['user_id'].max()+1\n",
    "num_items = data_30['item_id'].max()+1\n",
    "all_movieIds = data['item_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforced Co-Training Framework\n",
    "\n",
    "Define classifiers: \n",
    "\n",
    "* $C_1, C_2$\n",
    "* Parition documents into $K$ subsets based similarity\n",
    "* Feed classifiers representative document concat user embedding\n",
    "* Based on classifer output build initial state $s_0$, concat of output from $C_1,C_2$,\n",
    "* feed state to Q-learner\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partioning by LSH\n",
    "\n",
    "https://santhoshhari.github.io/Locality-Sensitive-Hashing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashTable:\n",
    "    def __init__(self, hash_size, inp_dimensions):\n",
    "        self.hash_size = hash_size\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.hash_table = dict()\n",
    "        self.projections = np.random.randn(self.hash_size, inp_dimensions)\n",
    "        self.subset_counter = 0\n",
    "        \n",
    "    def generate_hash(self, inp_vector):\n",
    "        bools = (np.dot(inp_vector, self.projections.T) > 0).astype('int')\n",
    "        return ''.join(bools.astype('str'))\n",
    "\n",
    "    def __setitem__(self, inp_vec, label):\n",
    "        hash_value = self.generate_hash(inp_vec)\n",
    "        if hash_value not in self.hash_table:\n",
    "            self.hash_table[hash_value] = self.hash_table\\\n",
    "                .get(hash_value, list()) + [self.subset_counter,label]\n",
    "            self.subset_counter+=1\n",
    "        else:\n",
    "            self.hash_table[hash_value] = self.hash_table\\\n",
    "                .get(hash_value, list()) + [label]\n",
    "        \n",
    "    def __getitem__(self, inp_vec):\n",
    "        hash_value = self.generate_hash(inp_vec)\n",
    "        return self.hash_table.get(hash_value, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, num_tables, hash_size, inp_dimensions):\n",
    "        self.num_tables = num_tables\n",
    "        self.hash_size = hash_size\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.hash_tables = list()\n",
    "        for i in range(self.num_tables):\n",
    "            self.hash_tables.append(HashTable(self.hash_size, self.inp_dimensions))\n",
    "    \n",
    "    def __setitem__(self, inp_vec, label):\n",
    "        for table in self.hash_tables:\n",
    "            table[inp_vec] = label\n",
    "    \n",
    "    def __getitem__(self, inp_vec):\n",
    "        results = list()\n",
    "        for table in self.hash_tables:\n",
    "            results.extend(table[inp_vec])\n",
    "        return list(set(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating movie LSH took 1.2041378021240234 seconds.\n"
     ]
    }
   ],
   "source": [
    "movie_hasher = HashTable(hash_size=10, inp_dimensions=19)\n",
    "start = time.time()\n",
    "for i,row in item_data.iterrows():\n",
    "    movie_label = row['movie id']\n",
    "    movie_feature_vec = item_data.iloc[i, 5:]\n",
    "    movie_hasher.__setitem__(movie_feature_vec, movie_label)\n",
    "end = time.time()\n",
    "print(f'Creating movie LSH took {end-start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_hasher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_feature_vec.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_hasher.hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we have item_data and a user_id\n",
    "# # Generate Labeled seeding data by\n",
    "# # for each item_id associated with current user get from subset\n",
    "np.random.seed(272)\n",
    "co_train_user = np.random.randint(0,num_users)\n",
    "\n",
    "\n",
    "# u_1|1, ... u_k|0 etc\n",
    "# This is L the seeding dataset\n",
    "def create_L(co_train_user, df):\n",
    "    co_train_user_ratings = df[df['user_id'] == co_train_user]\n",
    "    co_train_u_ks = np.zeros(shape=len(movie_hasher.hash_table),dtype=int)\n",
    "    L_item_ids = np.zeros(shape=len(movie_hasher.hash_table),dtype=int)\n",
    "    user_ids = np.ones(shape=len(movie_hasher.hash_table),dtype=int)*co_train_user\n",
    "    for i, row in co_train_user_ratings.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        item_vec = item_data[item_data['movie id'] == item_id].iloc[:,5:].values[0]\n",
    "        subset_id = movie_hasher.__getitem__(item_vec)[0]\n",
    "        co_train_u_ks[subset_id] = 1\n",
    "        L_item_ids[subset_id] = item_id\n",
    "    return user_ids, L_item_ids, co_train_u_ks\n",
    "\n",
    "L_user_ids, L_item_ids, L_u_ks = create_L(co_train_user, train_ratings)\n",
    "L_val_user_ids, L_val_item_ids, L_val_u_ks = create_L(co_train_user, test_ratings)\n",
    "# consists of current user_id|positive examples|negative exapmples\n",
    "# Generate labeled val data from test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
       "       242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,   0])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(L_user_ids,[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1 = Classifier(num_users,num_items)\n",
    "# c2 = Classifier(num_users,num_items)\n",
    "# pred = np.round(c1(torch.tensor(L_user_ids), torch.tensor(L_item_ids)).squeeze(1).detach().numpy()) == L_u_ks\n",
    "# pred.mean(),L_u_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Producing datasets for cotraining process\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, users, item_ids, u_k_labels):\n",
    "        self.users = torch.tensor(users)\n",
    "        self.items =  torch.tensor(item_ids)\n",
    "        self.labels = torch.tensor(u_k_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2</td>\n",
       "      <td>292</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37026</th>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36334</th>\n",
       "      <td>2</td>\n",
       "      <td>309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35981</th>\n",
       "      <td>2</td>\n",
       "      <td>310</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35337</th>\n",
       "      <td>2</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38670</th>\n",
       "      <td>3</td>\n",
       "      <td>322</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14381</th>\n",
       "      <td>3</td>\n",
       "      <td>327</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40899</th>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14654</th>\n",
       "      <td>3</td>\n",
       "      <td>307</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41910</th>\n",
       "      <td>3</td>\n",
       "      <td>271</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating\n",
       "700          2      292       4\n",
       "37026        2      306       4\n",
       "36334        2      309       1\n",
       "35981        2      310       4\n",
       "35337        2      294       1\n",
       "...        ...      ...     ...\n",
       "38670        3      322       3\n",
       "14381        3      327       4\n",
       "40899        3      352       2\n",
       "14654        3      307       3\n",
       "41910        3      271       3\n",
       "\n",
       "[114 rows x 3 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratings[train_ratings['user_id'].isin([2,3])].sort_values(by='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should extend to have multiple users\n",
    "class CotrainEnv(gym.Env):\n",
    "    #metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, c1, c2):\n",
    "        super(CotrainEnv, self).__init__()\n",
    "        self.co_train_user = None\n",
    "        #sample representatives v[1] because I used v[0] to mark subset number\n",
    "        self.sample_reps = [ v[1] for v in movie_hasher.hash_table.values() ]\n",
    "        self.u_subs = [ v[1:] for v in movie_hasher.hash_table.values() ]\n",
    "        self.k = len(self.sample_reps)\n",
    "        self.obs_shape = (2*self.k,)\n",
    "        self.action_space = spaces.Discrete(self.k)# one of the k subsets\n",
    "        self.observation_space = spaces.Box(low=0, high=1,\n",
    "                                        shape=self.obs_shape)\n",
    "        self.c1, self.c2 = c1,c2\n",
    "        self.c1_optim = torch.optim.Adam(self.c1.parameters())\n",
    "        self.c2_optim = torch.optim.Adam(self.c2.parameters())\n",
    "\n",
    "        self.t = 0\n",
    "        self.num_steps = 10\n",
    "        self.c1_losses = []\n",
    "        self.c2_losses = []\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # accuracy t-1\n",
    "        L_val_user_ids, L_val_item_ids, L_val_u_ks = self.create_L(self.co_train_user, test_ratings)\n",
    "        L_val_user_ids = torch.tensor(L_val_user_ids)\n",
    "        L_val_item_ids = torch.tensor(L_val_item_ids)\n",
    "        L_val_u_ks = torch.tensor(L_val_u_ks)\n",
    "\n",
    "        prev_c1_label = np.round(self.c1(L_val_user_ids,L_val_item_ids).detach().numpy())\n",
    "        prev_c2_label = np.round(self.c2(L_val_user_ids,L_val_item_ids).detach().numpy())\n",
    "        \n",
    "        # action is subset to sample from..\n",
    "        # Create \n",
    "        u_at = self.u_subs[action]\n",
    "        self.cotrain(self.c1, self.c2, u_at)\n",
    "        \n",
    "        # accuracy t\n",
    "\n",
    "        post_c1_label = np.round(self.c1(L_val_user_ids,L_val_item_ids).detach().numpy())\n",
    "        post_c2_label = np.round(self.c2(L_val_user_ids,L_val_item_ids).detach().numpy())\n",
    "        \n",
    "        reward = self.calc_reward(prev_c1_label, prev_c2_label, post_c1_label, post_c2_label, L_val_u_ks)\n",
    "        # update state\n",
    "        state_holder = []\n",
    "        for rep_sample in self.sample_reps:\n",
    "            c1_pred = self.c1(torch.tensor([self.co_train_user]),torch.tensor([rep_sample]))\n",
    "            prob_c1 = torch.sigmoid(c1_pred).detach().numpy()\n",
    "            c2_pred = self.c2(torch.tensor([self.co_train_user]),torch.tensor([rep_sample]))\n",
    "            prob_c2 = torch.sigmoid(c2_pred).detach().numpy()\n",
    "            state_holder+=[prob_c1,prob_c2]\n",
    "        observation = np.concatenate(state_holder).reshape(-1)\n",
    "        done = self.t == self.num_steps\n",
    "        self.t+=1\n",
    "        info = {}\n",
    "        self.report_losses()\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def report_losses(self):\n",
    "        if self.t % 10 == 0:\n",
    "            print('Current user: {}'.format(self.co_train_user))\n",
    "            print('====> C1 loss: {:.4f}'.format(np.mean(self.c1_losses)))\n",
    "            print('====> C2 loss: {:.4f}'.format(np.mean(self.c2_losses)))\n",
    "    \n",
    "    def create_L(self, co_train_user, df):\n",
    "        # Change this to have 4 negative examples for every positive\n",
    "        num_neg = 4\n",
    "        co_train_user_ratings = df[df['user_id']==co_train_user]#.sort_values(by='user_id')\n",
    "        #maybe pass movie_hasher to env as well\n",
    "        L_labels = []\n",
    "        L_item_ids = []\n",
    "        user_ids = [co_train_user]\n",
    "        items_user_liked = set()\n",
    "        all_movie_ids = set(item_data['movie id'].values)\n",
    "        #current_user_id = co_train_user[0]\n",
    "        for i, row in co_train_user_ratings.iterrows():\n",
    "#             if current_user_id != row['user_id']:\n",
    "#                 #reset items liked\n",
    "#                 items_user_liked.clear()\n",
    "            item_id = row['item_id']\n",
    "            items_user_liked.add(item_id)\n",
    "           \n",
    "            L_labels.append(1)\n",
    "            L_item_ids.append(item_id)\n",
    "        # Add negative item examples\n",
    "        movie_selection_set = all_movie_ids - items_user_liked\n",
    "        for _ in range(num_neg):\n",
    "            movie_selection_list = list(movie_selection_set)\n",
    "            negative_example = np.random.choice(movie_selection_list, size=1)[0]\n",
    "            L_item_ids.append(negative_example)\n",
    "            L_labels.append(0)\n",
    "            movie_selection_set.remove(negative_example)\n",
    "              \n",
    "        return user_ids*len(L_item_ids), L_item_ids, L_labels\n",
    "    \n",
    "    def train_classifier(self, model, optimizer, dataloader, model_name, loss_collater):\n",
    "        model.train()\n",
    "        for data in dataloader:\n",
    "            user_input, item_input, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            predicted_labels = model(user_input, item_input)\n",
    "            #pos_weight= num_negative examples torch.tensor([4.])\n",
    "            loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.]))(predicted_labels, labels.view(-1, 1).float())\n",
    "            loss_collater.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#         if self.t % 10==0:\n",
    "#             print('====> {} Final loss: {:.4f}'.format(model_name,loss.item()))\n",
    "    \n",
    "    def train_on_L(self):\n",
    "        L_user_ids, L_item_ids, L_u_ks = self.create_L(self.co_train_user, train_ratings)\n",
    "        # fit c1 on L\n",
    "        L = CoTrainDataset(L_user_ids, L_item_ids, L_u_ks)\n",
    "        L_dataloader = DataLoader(L, batch_size=16, num_workers=0)\n",
    "        self.train_classifier(self.c1,self.c1_optim, L_dataloader, 'c1 L-train', self.c1_losses)\n",
    "        \n",
    "        # fit c2 on L\n",
    "        self.train_classifier(self.c2, self.c2_optim, L_dataloader, 'c2 L-train', self.c2_losses)\n",
    "        return\n",
    "    \n",
    "    def cotrain(self, c1, c2, u_at):\n",
    "        L_user_ids, L_item_ids, L_u_ks = self.create_L(self.co_train_user, train_ratings)\n",
    "        L = CoTrainDataset(L_user_ids, L_item_ids, L_u_ks)\n",
    "        user_input = [self.co_train_user]*len(u_at)\n",
    "        \n",
    "        # Used for u_at + L dataset\n",
    "        u_L_user_ids = np.append(L_user_ids, user_input)\n",
    "        u_L_item_ids = np.append(L_item_ids, u_at)\n",
    "        \n",
    "        # u_at is subset\n",
    "        # c1 labels u_at then train c2 using that label\n",
    "        c1_preds = torch.sigmoid(c1(torch.tensor(user_input), torch.tensor(u_at))).detach().numpy()\n",
    "        u_at_c1_label = np.round(c1_preds)\n",
    "        # Make u_at + L dataset\n",
    "        u_at_c1_L_labels = np.append(L_u_ks, u_at_c1_label)\n",
    "        u_at_c1_dataset = CoTrainDataset(u_L_user_ids, u_L_item_ids, u_at_c1_L_labels)\n",
    "        u_at_c1_dataloader = DataLoader(u_at_c1_dataset, batch_size=4, num_workers=0)\n",
    "        self.train_classifier(self.c2, self.c2_optim, u_at_c1_dataloader, 'c1 co-train', self.c2_losses)\n",
    "        \n",
    "        # c2 labels u_at then train c1 using that label\n",
    "        c2_preds = torch.sigmoid(c2(torch.tensor(user_input), torch.tensor(u_at))).detach().numpy()\n",
    "        u_at_c2_label = np.round(c2_preds)\n",
    "        u_at_c2_L_labels = np.append(L_u_ks, u_at_c2_label)\n",
    "        u_at_c2_dataset = CoTrainDataset(u_L_user_ids, u_L_item_ids, u_at_c2_L_labels)\n",
    "        u_at_c2_dataloader = DataLoader(u_at_c2_dataset, batch_size=4, num_workers=0)\n",
    "        self.train_classifier(self.c1,self.c1_optim, u_at_c2_dataloader, 'c2 co-train', self.c1_losses)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def calc_reward(self, prev_c1, prev_c2, post_c1, post_c2, true_labels):\n",
    "        post_c1, post_c2 = post_c1.squeeze(1),post_c2.squeeze(1)\n",
    "        prev_c1, prev_c2 = prev_c1.squeeze(1), prev_c2.squeeze(1)\n",
    "        assert post_c1.shape == true_labels.shape\n",
    "        assert post_c2.shape == true_labels.shape\n",
    "        assert prev_c1.shape == true_labels.shape\n",
    "        assert prev_c2.shape == true_labels.shape\n",
    "       \n",
    "        true_labels_np = true_labels.detach().numpy()\n",
    "\n",
    "        r_1 = np.mean(post_c1 == true_labels_np) - np.mean(prev_c1 == true_labels_np)\n",
    "        r_2 = np.mean(post_c2 == true_labels_np) - np.mean(prev_c2 == true_labels_np)\n",
    "        r = r_1*r_2\n",
    "        reward = r if r > 0 else 0\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.c1_losses = []\n",
    "        self.c2_losses = []\n",
    "        self.co_train_user = np.random.choice(np.arange(num_users),size=1)[0]#np.random.randint(0,num_users)\n",
    "        observation = np.ones(shape=self.obs_shape)*0.5\n",
    "        self.train_on_L()\n",
    "        return observation  # reward, done, info can't be included\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close (self):\n",
    "         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classifier\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\" Neural Collaborative Filtering (NCF)\n",
    "    \n",
    "        Args:\n",
    "            num_users (int): Number of unique users\n",
    "            num_items (int): Number of unique items\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, movie_onehot):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n",
    "        self.movie_onehot = movie_onehot\n",
    "        self.one_hot_dim = len(movie_onehot[1])\n",
    "        input_dim = 16 + self.one_hot_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.output = nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "    def forward(self, user_input, item_input):\n",
    "        #print(item_input)\n",
    "        #Build batch of movie features\n",
    "        movie_features = []\n",
    "        for item_id in item_input:\n",
    "            movie_features.append(self.movie_onehot[item_id.item()].astype('float'))\n",
    "        # Pass through embedding layers\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "        #print(movie_features)\n",
    "        movie_features = torch.tensor(movie_features, requires_grad=True,dtype=torch.float)\n",
    "        # Concat the two embedding layers\n",
    "        vector = torch.cat([user_embedded, item_embedded,movie_features], dim=-1)\n",
    "\n",
    "        # Pass through dense layer\n",
    "        vector = nn.ReLU()(self.fc1(vector))\n",
    "        vector = nn.ReLU()(self.fc2(vector))\n",
    "\n",
    "        # Output layer\n",
    "        pred = self.output(vector)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "def train_classifier(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    for data in dataloader:\n",
    "        user_input, item_input, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        predicted_labels = model(user_input, item_input)\n",
    "        loss = nn.BCEWithLogitsLoss()(predicted_labels, labels.view(-1, 1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('====>Final loss: {:.4f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = Classifier(num_users,num_items, movie_to_onehot)\n",
    "c2 = Classifier(num_users,num_items,movie_to_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = CotrainEnv(c1, c2)\n",
    "#check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import DiscreteCQL\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy\n",
    "cql = DiscreteCQL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-20 18:52.23 [info     ] Directory is created at d3rlpy_logs/DiscreteCQL_online_20210520185223\n",
      "2021-05-20 18:52.23 [debug    ] Building model...\n",
      "2021-05-20 18:52.23 [debug    ] Model has been built.\n",
      "2021-05-20 18:52.23 [info     ] Parameters are saved to d3rlpy_logs/DiscreteCQL_online_20210520185223/params.json params={'action_scaler': None, 'augmentation': {'params': {'n_mean': 1}, 'augmentations': []}, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'bootstrap': False, 'share_encoder': False}}, 'real_ratio': 1.0, 'scaler': None, 'target_reduction_type': 'min', 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DiscreteCQL', 'observation_shape': (282,), 'action_size': 141}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:02<00:20,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: 894\n",
      "====> C1 loss: 0.2323\n",
      "====> C2 loss: 0.2268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:03<00:07,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: 283\n",
      "====> C1 loss: 0.2949\n",
      "====> C2 loss: 0.3062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:05<00:10,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: 693\n",
      "====> C1 loss: 0.1714\n",
      "====> C2 loss: 0.1689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:08<00:12,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: 774\n",
      "====> C1 loss: 0.1128\n",
      "====> C2 loss: 0.1189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:10<00:05,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: 835\n",
      "====> C1 loss: 0.2168\n",
      "====> C2 loss: 0.2353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:11<00:03,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: 582\n",
      "====> C1 loss: 0.3352\n",
      "====> C2 loss: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:12<00:01,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: 146\n",
      "====> C1 loss: 0.3753\n",
      "====> C2 loss: 0.3738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [00:13<00:00, 11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: 572\n",
      "====> C1 loss: 0.5660\n",
      "====> C2 loss: 0.5983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# # setup replay buffer\n",
    "# buffer = ReplayBuffer(maxlen=1000, env=env)\n",
    "# # setup explorers\n",
    "# explorer = LinearDecayEpsilonGreedy(start_epsilon=1.0,\n",
    "#                                     end_epsilon=0.01,\n",
    "#                                     duration=100)\n",
    "# cql.fit_online(env,buffer=buffer, explorer=explorer, n_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#buffer.to_mdp_dataset().rewards\n",
    "#buffer.to_mdp_dataset().observations[-1]\n",
    "#buffer.to_mdp_dataset().actions\n",
    "buffer.to_mdp_dataset().episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Reinforced Co-Trainer \n",
    "\n",
    "* Measure Hit Rate@10\n",
    "\n",
    "* Will implement NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (user_embedding): Embedding(944, 8)\n",
       "  (item_embedding): Embedding(1683, 8)\n",
       "  (fc1): Linear(in_features=35, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load classifiers\n",
    "# Bring Images into same latent space, also preprocess\n",
    "device = torch.device('cpu')\n",
    "\n",
    "checkpoint = torch.load('classifiers_15000.pt',map_location=device)\n",
    "c1.load_state_dict(checkpoint['c1_state_dict'])\n",
    "c2.load_state_dict(checkpoint['c2_state_dict'])\n",
    "\n",
    "c1.eval()\n",
    "c2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hit Ratio @ 10 is 0.3913\n"
     ]
    }
   ],
   "source": [
    "# User-item pairs for testing\n",
    "test_user_item_set = set(zip(test_ratings['user_id'], test_ratings['item_id']))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = data_30.groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "\n",
    "hits = []\n",
    "for (u,i) in test_user_item_set:\n",
    "    interacted_items = user_interacted_items[u]\n",
    "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
    "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "    test_items = selected_not_interacted + [i]\n",
    "    \n",
    "    c1_predicted_labels = c1(torch.tensor([u]*100), torch.tensor(test_items))\n",
    "    \n",
    "    c2_predicted_labels = c2(torch.tensor([u]*100), torch.tensor(test_items))\n",
    "    logits_labels = torch.sigmoid((c1_predicted_labels+c2_predicted_labels)/2)\n",
    "    predicted_labels = np.squeeze(logits_labels.detach().numpy())\n",
    "    \n",
    "    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    if i in top10_items:\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits.append(0)\n",
    "        \n",
    "print(\"The Hit Ratio @ 10 is {:.4f}\".format(np.average(hits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Trainer + NCF\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
