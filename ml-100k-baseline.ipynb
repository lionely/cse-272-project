{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
      "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
      "Done! Dataset ml-100k has been saved to /Users/newtype/.surprise_data/ml-100k\n"
     ]
    }
   ],
   "source": [
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fbf1ba096a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.938935046586064"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation by Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_ml100k():\n",
    "    data_dir = 'ml-100k/'\n",
    "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    data = pd.read_csv(os.path.join(data_dir, 'u.data'), '\\t', names=names,\n",
    "                       engine='python')\n",
    "    num_users = data.user_id.unique().shape[0]\n",
    "    num_items = data.item_id.unique().shape[0]\n",
    "    return data, num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, num_users, num_items = read_data_ml100k()\n",
    "data_30 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave one out split\n",
    "data_30['rank_latest'] = data_30.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "\n",
    "train_ratings = data_30[data_30['rank_latest'] != 1]\n",
    "test_ratings = data_30[data_30['rank_latest'] == 1]\n",
    "\n",
    "# drop columns that we no longer need\n",
    "train_ratings = train_ratings[['user_id', 'item_id', 'rating']]\n",
    "test_ratings = test_ratings[['user_id', 'item_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = data_30['user_id'].max()+1\n",
    "num_items = data_30['item_id'].max()+1\n",
    "all_movieIds = data['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hit_rate(at=10):\n",
    "    # User-item pairs for testing\n",
    "    test_user_item_set = set(zip(test_ratings['user_id'], test_ratings['item_id']))\n",
    "\n",
    "    # Dict of all items that are interacted with by each user\n",
    "    user_interacted_items = data_30.groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "\n",
    "    hits = []\n",
    "    for (u,i) in test_user_item_set:\n",
    "        interacted_items = user_interacted_items[u]\n",
    "        not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
    "        selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "        test_items = selected_not_interacted + [i]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for j in range(100):\n",
    "            rating_pred = algo.predict(u, test_items[j])[3]\n",
    "            predicted_labels.append(rating_pred)\n",
    "        #print(predicted_labels)\n",
    "        top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:at].tolist()]\n",
    "        \n",
    "        if i in top10_items:\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "\n",
    "    print(\"\\nThe Hit Ratio @ {} is {:.4f}\".format(at, np.average(hits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Hit Ratio @ 1 is 1.0000\n",
      "\n",
      "The Hit Ratio @ 5 is 1.0000\n",
      "\n",
      "The Hit Ratio @ 10 is 1.0000\n"
     ]
    }
   ],
   "source": [
    "find_hit_rate(at=1)\n",
    "find_hit_rate(at=5)\n",
    "find_hit_rate(at=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
